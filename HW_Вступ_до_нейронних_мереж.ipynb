{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLHTNfSclli"
   },
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOYB-RHfc_r"
   },
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3BNXSR-VdYKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QLKZ77x4v_-v"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KjoeaDrk6fO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKzbJKfOgGV8"
   },
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aXhKw6Tdj1-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x301ca11b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eApcB7eb6h9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
      "tensor([0.6213], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(1, 3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGxNGTaf5s6"
   },
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pSz2j4Fh6jBv"
   },
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return 1 / (1 + torch.exp(-(x @ w.t() + b))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs, w, b)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всі передбачення =1. Це виглядає підозріло. Так могло статися через те що xw+b велике, тоді exp(-(xw+b)) = 1/(xw+b), а 1 поділити на велике число буде дорівнювати бизько 0. Тоді в результаті 1/(1+0)=1. Треба підбирати інші ваги. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   },
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1bWlovvx6kZS"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "    loss = -(true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nan тому що логарифм 0 не визначений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKpQxdHi1__"
   },
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YAbXUNSJ6mCl"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
      "tensor([[nan, nan, nan]])\n"
     ]
    }
   ],
   "source": [
    "# Градієнти вагів\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6213], requires_grad=True)\n",
      "tensor([nan])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDN1t1RujQsK"
   },
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPSQyttpVjO"
   },
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-EBOJ3tsnRaD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
      "tensor([0.0006], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
    "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-JwXiSpX6orh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5174],\n",
      "        [0.5220],\n",
      "        [0.5244],\n",
      "        [0.5204],\n",
      "        [0.5190]], grad_fn=<MulBackward0>)\n",
      "tensor(0.6829, grad_fn=<MeanBackward0>)\n",
      "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
      "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
      "tensor([0.0006], requires_grad=True)\n",
      "tensor([-0.0794])\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs, w, b)\n",
    "print(preds)\n",
    "\n",
    "loss = binary_cross_entropy(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Градієнти вагів\n",
    "print(w)\n",
    "print(w.grad)\n",
    "\n",
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCdi44IT334o"
   },
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "  1. Генерація прогнозів\n",
    "  2. Обчислення втрат\n",
    "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "  5. Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "mObHPyE06qsO"
   },
   "outputs": [],
   "source": [
    "# Тренування протягом 1000 епох\n",
    "for i in range(1000):\n",
    "    preds = model(inputs, w, b)\n",
    "    loss = binary_cross_entropy(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3357, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Обчислюю loss\n",
    "preds = model(inputs, w, b)\n",
    "loss = binary_cross_entropy(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5777],\n",
       "        [0.6685],\n",
       "        [0.9113],\n",
       "        [0.1616],\n",
       "        [0.8653]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передбачення доволі непогані, при порозі класифікації 0.5 алгоритм помиляється лише в першому зразку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuRhlyF9qAia"
   },
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IX8Bhm74rV4M"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X2dV30KtAPu"
   },
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "chrvMfBs6vjo"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMFaa8suOd3"
   },
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ZCsRo5Mx6wEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 69.,  96.,  70.],\n",
       "         [102.,  43.,  37.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymcQOo_hum6I"
   },
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EyAwhTBW6xxz"
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(3, 1)\n",
    "        self.act1 = nn.Sigmoid() # Activation function\n",
    "\n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflV7xeVyoJy"
   },
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "3QCATPU_6yfa"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=5e-4)\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = loss_fn(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (preds >= 0.5).int()\n",
    "print(preds)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds == targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделі не вдалось навчитись. Loss дуже великий, дуже багато помилок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-WrYnKzMzq"
   },
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "cEHQH9qE626k"
   },
   "outputs": [],
   "source": [
    "# Модифікована функцію fit для відстеження втрат\n",
    "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ініціалізуємо акумулятор для втрат\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            # Генеруємо передбачення\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Обчислюємо втрати\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Виконуємо градієнтний спуск\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Накопичуємо втрати\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Обчислюємо середні втрати для епохи\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Виводимо підсумок епохи\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 2.1634\n",
      "Epoch [20/1000], Loss: 0.3613\n",
      "Epoch [30/1000], Loss: 0.4098\n",
      "Epoch [40/1000], Loss: 0.2817\n",
      "Epoch [50/1000], Loss: 0.2458\n",
      "Epoch [60/1000], Loss: 0.3782\n",
      "Epoch [70/1000], Loss: 0.2610\n",
      "Epoch [80/1000], Loss: 0.1905\n",
      "Epoch [90/1000], Loss: 0.2249\n",
      "Epoch [100/1000], Loss: 0.1994\n",
      "Epoch [110/1000], Loss: 0.2383\n",
      "Epoch [120/1000], Loss: 0.2414\n",
      "Epoch [130/1000], Loss: 0.1727\n",
      "Epoch [140/1000], Loss: 0.1669\n",
      "Epoch [150/1000], Loss: 0.2119\n",
      "Epoch [160/1000], Loss: 0.1610\n",
      "Epoch [170/1000], Loss: 0.1643\n",
      "Epoch [180/1000], Loss: 0.1816\n",
      "Epoch [190/1000], Loss: 0.2047\n",
      "Epoch [200/1000], Loss: 0.1401\n",
      "Epoch [210/1000], Loss: 0.1676\n",
      "Epoch [220/1000], Loss: 0.1392\n",
      "Epoch [230/1000], Loss: 0.1398\n",
      "Epoch [240/1000], Loss: 0.1581\n",
      "Epoch [250/1000], Loss: 0.1324\n",
      "Epoch [260/1000], Loss: 0.1268\n",
      "Epoch [270/1000], Loss: 0.1362\n",
      "Epoch [280/1000], Loss: 0.1206\n",
      "Epoch [290/1000], Loss: 0.1169\n",
      "Epoch [300/1000], Loss: 0.1082\n",
      "Epoch [310/1000], Loss: 0.1256\n",
      "Epoch [320/1000], Loss: 0.1060\n",
      "Epoch [330/1000], Loss: 0.1079\n",
      "Epoch [340/1000], Loss: 0.1209\n",
      "Epoch [350/1000], Loss: 0.1106\n",
      "Epoch [360/1000], Loss: 0.0997\n",
      "Epoch [370/1000], Loss: 0.1136\n",
      "Epoch [380/1000], Loss: 0.1029\n",
      "Epoch [390/1000], Loss: 0.1047\n",
      "Epoch [400/1000], Loss: 0.0948\n",
      "Epoch [410/1000], Loss: 0.0874\n",
      "Epoch [420/1000], Loss: 0.1027\n",
      "Epoch [430/1000], Loss: 0.0977\n",
      "Epoch [440/1000], Loss: 0.0864\n",
      "Epoch [450/1000], Loss: 0.1153\n",
      "Epoch [460/1000], Loss: 0.0860\n",
      "Epoch [470/1000], Loss: 0.0879\n",
      "Epoch [480/1000], Loss: 0.0791\n",
      "Epoch [490/1000], Loss: 0.1085\n",
      "Epoch [500/1000], Loss: 0.0786\n",
      "Epoch [510/1000], Loss: 0.0844\n",
      "Epoch [520/1000], Loss: 0.0725\n",
      "Epoch [530/1000], Loss: 0.0820\n",
      "Epoch [540/1000], Loss: 0.0823\n",
      "Epoch [550/1000], Loss: 0.0724\n",
      "Epoch [560/1000], Loss: 0.0765\n",
      "Epoch [570/1000], Loss: 0.0895\n",
      "Epoch [580/1000], Loss: 0.0695\n",
      "Epoch [590/1000], Loss: 0.0719\n",
      "Epoch [600/1000], Loss: 0.0783\n",
      "Epoch [610/1000], Loss: 0.0689\n",
      "Epoch [620/1000], Loss: 0.0652\n",
      "Epoch [630/1000], Loss: 0.0659\n",
      "Epoch [640/1000], Loss: 0.0703\n",
      "Epoch [650/1000], Loss: 0.0628\n",
      "Epoch [660/1000], Loss: 0.0676\n",
      "Epoch [670/1000], Loss: 0.0620\n",
      "Epoch [680/1000], Loss: 0.0586\n",
      "Epoch [690/1000], Loss: 0.0607\n",
      "Epoch [700/1000], Loss: 0.0595\n",
      "Epoch [710/1000], Loss: 0.0565\n",
      "Epoch [720/1000], Loss: 0.0557\n",
      "Epoch [730/1000], Loss: 0.0573\n",
      "Epoch [740/1000], Loss: 0.0545\n",
      "Epoch [750/1000], Loss: 0.0595\n",
      "Epoch [760/1000], Loss: 0.0613\n",
      "Epoch [770/1000], Loss: 0.0609\n",
      "Epoch [780/1000], Loss: 0.0584\n",
      "Epoch [790/1000], Loss: 0.0532\n",
      "Epoch [800/1000], Loss: 0.0547\n",
      "Epoch [810/1000], Loss: 0.0504\n",
      "Epoch [820/1000], Loss: 0.0547\n",
      "Epoch [830/1000], Loss: 0.0511\n",
      "Epoch [840/1000], Loss: 0.0499\n",
      "Epoch [850/1000], Loss: 0.0499\n",
      "Epoch [860/1000], Loss: 0.0510\n",
      "Epoch [870/1000], Loss: 0.0483\n",
      "Epoch [880/1000], Loss: 0.0469\n",
      "Epoch [890/1000], Loss: 0.0476\n",
      "Epoch [900/1000], Loss: 0.0473\n",
      "Epoch [910/1000], Loss: 0.0497\n",
      "Epoch [920/1000], Loss: 0.0505\n",
      "Epoch [930/1000], Loss: 0.0480\n",
      "Epoch [940/1000], Loss: 0.0444\n",
      "Epoch [950/1000], Loss: 0.0450\n",
      "Epoch [960/1000], Loss: 0.0446\n",
      "Epoch [970/1000], Loss: 0.0471\n",
      "Epoch [980/1000], Loss: 0.0445\n",
      "Epoch [990/1000], Loss: 0.0435\n",
      "Epoch [1000/1000], Loss: 0.0435\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 1000 epochs\n",
    "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds = (preds >= 0.5).int()\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds == targets"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
